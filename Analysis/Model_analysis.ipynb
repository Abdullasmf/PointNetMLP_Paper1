{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Model Comparison and Analysis\n",
    "\n",
    "This notebook compares the performance of all trained models on both L_bracket and Plate_hole datasets.\n",
    "\n",
    "## Models Compared:\n",
    "1. **PointNetMLPJoint** - Joint encoder-decoder architecture\n",
    "2. **DenseNoFFT** - Dense concatenation without Fourier features\n",
    "3. **SpectralDeepONet** - DeepONet with Fourier features\n",
    "4. **VanillaDeepONet** - Standard DeepONet without Fourier features\n",
    "\n",
    "## Datasets:\n",
    "- **L_bracket** - L-shaped bracket geometry\n",
    "- **Plate_hole** - Plate with hole geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from pn_models import PointNetMLPJoint\n",
    "from benchmarks import VanillaDeepONet, SpectralDeepONet, DenseNoFFT\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directory paths\n",
    "current_dir = os.getcwd()\n",
    "repo_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Data paths\n",
    "L_bracket_data_path = Path(repo_dir, 'L_Bracket', 'L_bracket_stress.h5')\n",
    "Plate_hole_data_path = Path(repo_dir, 'Plate_Hole', 'Plate_hole_stress.h5')\n",
    "\n",
    "# Model directories\n",
    "model_dirs = {\n",
    "    'PointNetMLPJoint': Path(repo_dir, 'PointNetMLPJoint', 'Trained_models'),\n",
    "    'DenseNoFFT': Path(repo_dir, 'DenseNoFFT', 'Trained_models'),\n",
    "    'SpectralDeepONet': Path(repo_dir, 'SpectralDeepONet', 'Trained_models'),\n",
    "    'VanillaDeepONet': Path(repo_dir, 'VanillaDeepONet', 'Trained_models')\n",
    "}\n",
    "\n",
    "# Example geometry paths\n",
    "L_bracket_example_path = Path(current_dir, 'L_bracket_example.h5')\n",
    "Plate_hole_example_path = Path(current_dir, 'Plate_hole_example.h5')\n",
    "\n",
    "print('Data paths configured:')\n",
    "print(f'  L_bracket data: {L_bracket_data_path.exists()}')\n",
    "print(f'  Plate_hole data: {Plate_hole_data_path.exists()}')\n",
    "print(f'  L_bracket example: {L_bracket_example_path.exists()}')\n",
    "print(f'  Plate_hole example: {Plate_hole_example_path.exists()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper_functions",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data_fn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5_data(path):\n",
    "    \"\"\"Load all samples from HDF5 file.\"\"\"\n",
    "    samples = []\n",
    "    with h5py.File(path, 'r') as hf:\n",
    "        keys = sorted(hf.keys(), key=lambda x: int(x.split('_')[1]))\n",
    "        for key in keys:\n",
    "            group = hf[key]\n",
    "            sample = {\n",
    "                'points': group['points'][:],  # (N, 2)\n",
    "                'stress': group['stress'][:],  # (N, 1)\n",
    "            }\n",
    "            # Stack for tensor conversion\n",
    "            coord_stress = np.hstack((sample['points'], sample['stress']))  # (N, 3)\n",
    "            samples.append(torch.from_numpy(coord_stress).float())\n",
    "    return samples\n",
    "\n",
    "def get_validation_split(samples, test_size=0.2, random_state=42):\n",
    "    \"\"\"Split data into train and validation sets (matching training script).\"\"\"\n",
    "    n_samples = len(samples)\n",
    "    idxs = list(range(n_samples))\n",
    "    train_idx, val_idx = train_test_split(idxs, test_size=test_size, random_state=random_state)\n",
    "    val_samples = [samples[i] for i in val_idx]\n",
    "    return val_samples, val_idx\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_loader_fn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_checkpoint(model_path, model_type):\n",
    "    \"\"\"Load a model from checkpoint using saved architecture config.\"\"\"\n",
    "    ckpt = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Extract architecture config\n",
    "    arch = ckpt['arch']\n",
    "    \n",
    "    # Create model based on type\n",
    "    if model_type == 'PointNetMLPJoint':\n",
    "        model = PointNetMLPJoint(\n",
    "            encoder_cfg=arch['encoder_cfg'],\n",
    "            head_hidden=arch['head_hidden']\n",
    "        )\n",
    "    elif model_type == 'DenseNoFFT':\n",
    "        model = DenseNoFFT(\n",
    "            latent_dim=arch['encoder_cfg']['latent_dim'],\n",
    "            head_hidden=arch['head_hidden'],\n",
    "            encoder_cfg=arch['encoder_cfg']\n",
    "        )\n",
    "    elif model_type == 'SpectralDeepONet':\n",
    "        model = SpectralDeepONet(\n",
    "            latent_dim=arch['encoder_cfg']['latent_dim'],\n",
    "            basis_dim=arch.get('basis_dim', 128),\n",
    "            branch_hidden=arch.get('branch_hidden', [256, 256]),\n",
    "            trunk_hidden=arch.get('trunk_hidden', [256, 256]),\n",
    "            encoder_cfg=arch['encoder_cfg']\n",
    "        )\n",
    "    elif model_type == 'VanillaDeepONet':\n",
    "        model = VanillaDeepONet(\n",
    "            latent_dim=arch['encoder_cfg']['latent_dim'],\n",
    "            basis_dim=arch.get('basis_dim', 128),\n",
    "            branch_hidden=arch.get('branch_hidden', [256, 256]),\n",
    "            trunk_hidden=arch.get('trunk_hidden', [256, 256]),\n",
    "            encoder_cfg=arch['encoder_cfg']\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model type: {model_type}')\n",
    "    \n",
    "    # Load state dict\n",
    "    model.load_state_dict(ckpt['model_state'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Extract normalization params\n",
    "    norm_params = {\n",
    "        'coord_center': ckpt['coord_center'].to(device),\n",
    "        'coord_half_range': ckpt['coord_half_range'].to(device),\n",
    "        'stress_mean': ckpt['stress_mean'].to(device),\n",
    "        'stress_std': ckpt['stress_std'].to(device)\n",
    "    }\n",
    "    \n",
    "    return model, norm_params, ckpt\n",
    "\n",
    "print('Model loading function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_fn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_samples, norm_params):\n",
    "    \"\"\"Evaluate model on validation samples.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in val_samples:\n",
    "            sample = sample.to(device)\n",
    "            points = sample[:, :2]  # (N, 2)\n",
    "            stress = sample[:, 2:3]  # (N, 1)\n",
    "            \n",
    "            # Normalize inputs\n",
    "            points_norm = (points - norm_params['coord_center']) / norm_params['coord_half_range']\n",
    "            stress_norm = (stress - norm_params['stress_mean']) / norm_params['stress_std']\n",
    "            \n",
    "            # Add batch dimension\n",
    "            geom_points = points_norm.unsqueeze(0)  # (1, N, 2)\n",
    "            query_points = points_norm.unsqueeze(0)  # (1, N, 2)\n",
    "            \n",
    "            # Predict\n",
    "            pred_norm = model(geom_points, query_points)  # (1, N, 1)\n",
    "            \n",
    "            # Denormalize predictions\n",
    "            pred = pred_norm * norm_params['stress_std'] + norm_params['stress_mean']\n",
    "            \n",
    "            all_preds.append(pred.squeeze().cpu().numpy())\n",
    "            all_targets.append(stress.squeeze().cpu().numpy())\n",
    "    \n",
    "    # Concatenate all predictions and targets\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "    mae = np.mean(np.abs(all_targets - all_preds))\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2\n",
    "    }\n",
    "\n",
    "print('Evaluation function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lbracket_section",
   "metadata": {},
   "source": [
    "# L_bracket Dataset Comparison\n",
    "\n",
    "Comparing all models trained on the L_bracket dataset using the same validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lbracket_load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load L_bracket data\n",
    "print('Loading L_bracket dataset...')\n",
    "L_bracket_samples = load_h5_data(L_bracket_data_path)\n",
    "L_bracket_val_samples, L_bracket_val_idx = get_validation_split(L_bracket_samples)\n",
    "print(f'Total samples: {len(L_bracket_samples)}')\n",
    "print(f'Validation samples: {len(L_bracket_val_samples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lbracket_eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all L_bracket models\n",
    "print('Evaluating L_bracket models...\\n')\n",
    "L_bracket_results = {}\n",
    "\n",
    "for model_name in ['PointNetMLPJoint', 'DenseNoFFT', 'SpectralDeepONet', 'VanillaDeepONet']:\n",
    "    print(f'Evaluating {model_name}...')\n",
    "    model_path = list(model_dirs[model_name].glob('L-*.pt'))[0]\n",
    "    \n",
    "    try:\n",
    "        model, norm_params, ckpt = load_model_and_checkpoint(model_path, model_name)\n",
    "        metrics = evaluate_model(model, L_bracket_val_samples, norm_params)\n",
    "        metrics['Best_Val_Loss'] = ckpt.get('best_val_loss', None)\n",
    "        L_bracket_results[model_name] = metrics\n",
    "        print(f'  MSE: {metrics[\"MSE\"]:.4f}, R\u00b2: {metrics[\"R2\"]:.4f}\\n')\n",
    "    except Exception as e:\n",
    "        print(f'  Error: {e}\\n')\n",
    "        L_bracket_results[model_name] = {'Error': str(e)}\n",
    "\n",
    "print('L_bracket evaluation complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lbracket_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table for L_bracket\n",
    "L_bracket_df = pd.DataFrame(L_bracket_results).T\n",
    "L_bracket_df = L_bracket_df.round(4)\n",
    "print('\\n=== L_bracket Model Comparison ===')\n",
    "print(L_bracket_df.to_string())\n",
    "print(f'\\nBest Model (lowest MSE): {L_bracket_df[\"MSE\"].idxmin()}')\n",
    "print(f'Best Model (highest R\u00b2): {L_bracket_df[\"R2\"].idxmax()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lbracket_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize L_bracket results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MSE comparison\n",
    "axes[0].bar(L_bracket_df.index, L_bracket_df['MSE'])\n",
    "axes[0].set_ylabel('MSE (MPa\u00b2)')\n",
    "axes[0].set_title('L_bracket: Mean Squared Error')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# R\u00b2 comparison\n",
    "axes[1].bar(L_bracket_df.index, L_bracket_df['R2'])\n",
    "axes[1].set_ylabel('R\u00b2 Score')\n",
    "axes[1].set_title('L_bracket: R\u00b2 Score (higher is better)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].axhline(y=1.0, color='r', linestyle='--', alpha=0.5, label='Perfect Score')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plate_section",
   "metadata": {},
   "source": [
    "# Plate_hole Dataset Comparison\n",
    "\n",
    "Comparing all models trained on the Plate_hole dataset using the same validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plate_load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Plate_hole data\n",
    "print('Loading Plate_hole dataset...')\n",
    "Plate_hole_samples = load_h5_data(Plate_hole_data_path)\n",
    "Plate_hole_val_samples, Plate_hole_val_idx = get_validation_split(Plate_hole_samples)\n",
    "print(f'Total samples: {len(Plate_hole_samples)}')\n",
    "print(f'Validation samples: {len(Plate_hole_val_samples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plate_eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all Plate_hole models\n",
    "print('Evaluating Plate_hole models...\\n')\n",
    "Plate_hole_results = {}\n",
    "\n",
    "for model_name in ['PointNetMLPJoint', 'DenseNoFFT', 'SpectralDeepONet', 'VanillaDeepONet']:\n",
    "    print(f'Evaluating {model_name}...')\n",
    "    model_path = list(model_dirs[model_name].glob('H-*.pt'))[0]\n",
    "    \n",
    "    try:\n",
    "        model, norm_params, ckpt = load_model_and_checkpoint(model_path, model_name)\n",
    "        metrics = evaluate_model(model, Plate_hole_val_samples, norm_params)\n",
    "        metrics['Best_Val_Loss'] = ckpt.get('best_val_loss', None)\n",
    "        Plate_hole_results[model_name] = metrics\n",
    "        print(f'  MSE: {metrics[\"MSE\"]:.4f}, R\u00b2: {metrics[\"R2\"]:.4f}\\n')\n",
    "    except Exception as e:\n",
    "        print(f'  Error: {e}\\n')\n",
    "        Plate_hole_results[model_name] = {'Error': str(e)}\n",
    "\n",
    "print('Plate_hole evaluation complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plate_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table for Plate_hole\n",
    "Plate_hole_df = pd.DataFrame(Plate_hole_results).T\n",
    "Plate_hole_df = Plate_hole_df.round(4)\n",
    "print('\\n=== Plate_hole Model Comparison ===')\n",
    "print(Plate_hole_df.to_string())\n",
    "print(f'\\nBest Model (lowest MSE): {Plate_hole_df[\"MSE\"].idxmin()}')\n",
    "print(f'Best Model (highest R\u00b2): {Plate_hole_df[\"R2\"].idxmax()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plate_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Plate_hole results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MSE comparison\n",
    "axes[0].bar(Plate_hole_df.index, Plate_hole_df['MSE'])\n",
    "axes[0].set_ylabel('MSE (MPa\u00b2)')\n",
    "axes[0].set_title('Plate_hole: Mean Squared Error')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# R\u00b2 comparison\n",
    "axes[1].bar(Plate_hole_df.index, Plate_hole_df['R2'])\n",
    "axes[1].set_ylabel('R\u00b2 Score')\n",
    "axes[1].set_title('Plate_hole: R\u00b2 Score (higher is better)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].axhline(y=1.0, color='r', linestyle='--', alpha=0.5, label='Perfect Score')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_geom_section",
   "metadata": {},
   "source": [
    "# Test Geometry Visualization\n",
    "\n",
    "Visualizing how all models perform on test geometries with percentage difference calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_geom_helper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_example_geometry(path):\n",
    "    \"\"\"Load example geometry from HDF5 file.\"\"\"\n",
    "    with h5py.File(path, 'r') as hf:\n",
    "        # Get first sample\n",
    "        key = list(hf.keys())[0]\n",
    "        group = hf[key]\n",
    "        points = group['points'][:]  # (N, 2)\n",
    "        stress = group['stress'][:]  # (N, 1)\n",
    "    return torch.from_numpy(points).float(), torch.from_numpy(stress).float()\n",
    "\n",
    "def predict_on_geometry(model, points, norm_params):\n",
    "    \"\"\"Make predictions on a geometry.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        points = points.to(device)\n",
    "        \n",
    "        # Normalize\n",
    "        points_norm = (points - norm_params['coord_center']) / norm_params['coord_half_range']\n",
    "        \n",
    "        # Add batch dimension\n",
    "        geom_points = points_norm.unsqueeze(0)  # (1, N, 2)\n",
    "        query_points = points_norm.unsqueeze(0)  # (1, N, 2)\n",
    "        \n",
    "        # Predict\n",
    "        pred_norm = model(geom_points, query_points)  # (1, N, 1)\n",
    "        \n",
    "        # Denormalize\n",
    "        pred = pred_norm * norm_params['stress_std'] + norm_params['stress_mean']\n",
    "        \n",
    "    return pred.squeeze().cpu().numpy()\n",
    "\n",
    "def calculate_percentage_difference(pred, target):\n",
    "    \"\"\"Calculate percentage difference for each node.\"\"\"\n",
    "    # Avoid division by zero\n",
    "    mask = np.abs(target) > 1e-6\n",
    "    pct_diff = np.zeros_like(target)\n",
    "    pct_diff[mask] = 100 * (pred[mask] - target[mask]) / np.abs(target[mask])\n",
    "    return pct_diff\n",
    "\n",
    "print('Test geometry helper functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lbracket_test",
   "metadata": {},
   "source": [
    "## L_bracket Test Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lbracket_test_pred",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load L_bracket test geometry\n",
    "print('Loading L_bracket test geometry...')\n",
    "L_test_points, L_test_stress = load_example_geometry(L_bracket_example_path)\n",
    "print(f'Number of nodes: {L_test_points.shape[0]}')\n",
    "\n",
    "# Make predictions with all models\n",
    "L_test_predictions = {}\n",
    "for model_name in ['PointNetMLPJoint', 'DenseNoFFT', 'SpectralDeepONet', 'VanillaDeepONet']:\n",
    "    print(f'Predicting with {model_name}...')\n",
    "    model_path = list(model_dirs[model_name].glob('L-*.pt'))[0]\n",
    "    model, norm_params, _ = load_model_and_checkpoint(model_path, model_name)\n",
    "    pred = predict_on_geometry(model, L_test_points, norm_params)\n",
    "    L_test_predictions[model_name] = pred\n",
    "    \n",
    "print('L_bracket test predictions complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lbracket_test_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize L_bracket test results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Ground truth\n",
    "sc0 = axes[0].scatter(L_test_points[:, 0], L_test_points[:, 1], \n",
    "                      c=L_test_stress.squeeze(), cmap='jet', s=20)\n",
    "axes[0].set_title('Ground Truth', fontsize=14, fontweight='bold')\n",
    "axes[0].set_aspect('equal')\n",
    "plt.colorbar(sc0, ax=axes[0], label='Stress (MPa)')\n",
    "\n",
    "# Model predictions\n",
    "for idx, (model_name, pred) in enumerate(L_test_predictions.items(), start=1):\n",
    "    sc = axes[idx].scatter(L_test_points[:, 0], L_test_points[:, 1], \n",
    "                          c=pred, cmap='jet', s=20)\n",
    "    mse = np.mean((pred - L_test_stress.squeeze().numpy())**2)\n",
    "    axes[idx].set_title(f'{model_name}\\nMSE: {mse:.4f}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_aspect('equal')\n",
    "    plt.colorbar(sc, ax=axes[idx], label='Stress (MPa)')\n",
    "\n",
    "# Hide extra subplot\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.suptitle('L_bracket Test Geometry: Stress Predictions', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lbracket_pct_diff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize percentage differences for L_bracket\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "target = L_test_stress.squeeze().numpy()\n",
    "\n",
    "for idx, (model_name, pred) in enumerate(L_test_predictions.items()):\n",
    "    pct_diff = calculate_percentage_difference(pred, target)\n",
    "    \n",
    "    sc = axes[idx].scatter(L_test_points[:, 0], L_test_points[:, 1], \n",
    "                          c=pct_diff, cmap='RdBu_r', s=20, vmin=-50, vmax=50)\n",
    "    \n",
    "    mean_pct = np.mean(np.abs(pct_diff))\n",
    "    axes[idx].set_title(f'{model_name}\\nMean |%Diff|: {mean_pct:.2f}%', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_aspect('equal')\n",
    "    plt.colorbar(sc, ax=axes[idx], label='% Difference')\n",
    "\n",
    "plt.suptitle('L_bracket: Percentage Difference per Node', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print('\\n=== L_bracket Percentage Difference Statistics ===')\n",
    "for model_name, pred in L_test_predictions.items():\n",
    "    pct_diff = calculate_percentage_difference(pred, target)\n",
    "    print(f'\\n{model_name}:')\n",
    "    print(f'  Mean |%Diff|: {np.mean(np.abs(pct_diff)):.2f}%')\n",
    "    print(f'  Max |%Diff|: {np.max(np.abs(pct_diff)):.2f}%')\n",
    "    print(f'  Std %Diff: {np.std(pct_diff):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plate_test",
   "metadata": {},
   "source": [
    "## Plate_hole Test Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plate_test_pred",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Plate_hole test geometry\n",
    "print('Loading Plate_hole test geometry...')\n",
    "P_test_points, P_test_stress = load_example_geometry(Plate_hole_example_path)\n",
    "print(f'Number of nodes: {P_test_points.shape[0]}')\n",
    "\n",
    "# Make predictions with all models\n",
    "P_test_predictions = {}\n",
    "for model_name in ['PointNetMLPJoint', 'DenseNoFFT', 'SpectralDeepONet', 'VanillaDeepONet']:\n",
    "    print(f'Predicting with {model_name}...')\n",
    "    model_path = list(model_dirs[model_name].glob('H-*.pt'))[0]\n",
    "    model, norm_params, _ = load_model_and_checkpoint(model_path, model_name)\n",
    "    pred = predict_on_geometry(model, P_test_points, norm_params)\n",
    "    P_test_predictions[model_name] = pred\n",
    "    \n",
    "print('Plate_hole test predictions complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plate_test_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Plate_hole test results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Ground truth\n",
    "sc0 = axes[0].scatter(P_test_points[:, 0], P_test_points[:, 1], \n",
    "                      c=P_test_stress.squeeze(), cmap='jet', s=20)\n",
    "axes[0].set_title('Ground Truth', fontsize=14, fontweight='bold')\n",
    "axes[0].set_aspect('equal')\n",
    "plt.colorbar(sc0, ax=axes[0], label='Stress (MPa)')\n",
    "\n",
    "# Model predictions\n",
    "for idx, (model_name, pred) in enumerate(P_test_predictions.items(), start=1):\n",
    "    sc = axes[idx].scatter(P_test_points[:, 0], P_test_points[:, 1], \n",
    "                          c=pred, cmap='jet', s=20)\n",
    "    mse = np.mean((pred - P_test_stress.squeeze().numpy())**2)\n",
    "    axes[idx].set_title(f'{model_name}\\nMSE: {mse:.4f}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_aspect('equal')\n",
    "    plt.colorbar(sc, ax=axes[idx], label='Stress (MPa)')\n",
    "\n",
    "# Hide extra subplot\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.suptitle('Plate_hole Test Geometry: Stress Predictions', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plate_pct_diff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize percentage differences for Plate_hole\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "target = P_test_stress.squeeze().numpy()\n",
    "\n",
    "for idx, (model_name, pred) in enumerate(P_test_predictions.items()):\n",
    "    pct_diff = calculate_percentage_difference(pred, target)\n",
    "    \n",
    "    sc = axes[idx].scatter(P_test_points[:, 0], P_test_points[:, 1], \n",
    "                          c=pct_diff, cmap='RdBu_r', s=20, vmin=-50, vmax=50)\n",
    "    \n",
    "    mean_pct = np.mean(np.abs(pct_diff))\n",
    "    axes[idx].set_title(f'{model_name}\\nMean |%Diff|: {mean_pct:.2f}%', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_aspect('equal')\n",
    "    plt.colorbar(sc, ax=axes[idx], label='% Difference')\n",
    "\n",
    "plt.suptitle('Plate_hole: Percentage Difference per Node', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print('\\n=== Plate_hole Percentage Difference Statistics ===')\n",
    "for model_name, pred in P_test_predictions.items():\n",
    "    pct_diff = calculate_percentage_difference(pred, target)\n",
    "    print(f'\\n{model_name}:')\n",
    "    print(f'  Mean |%Diff|: {np.mean(np.abs(pct_diff)):.2f}%')\n",
    "    print(f'  Max |%Diff|: {np.max(np.abs(pct_diff)):.2f}%')\n",
    "    print(f'  Std %Diff: {np.std(pct_diff):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "# Summary and Conclusions\n",
    "\n",
    "This notebook compared all four models on both datasets:\n",
    "- Validation set performance\n",
    "- Test geometry predictions\n",
    "- Per-node percentage differences\n",
    "\n",
    "Key findings will be displayed above in the comparison tables and visualizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}